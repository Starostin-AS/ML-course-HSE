{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python\\anaconda\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "E:\\python\\anaconda\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk import stem\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn import cross_validation\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# С чем работаем?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим данные - прочтём и изучим их сбалансированность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       message\n",
      "label         \n",
      "0         4825\n",
      "1          747\n"
     ]
    }
   ],
   "source": [
    "path = 'smsspamcollection/SMSSpamCollection'\n",
    "mes = pandas.read_csv(path, sep='\\t',names=[\"label\", \"message\"])\n",
    "mes['label'] = mes['label'].map({'ham': 0, 'spam': 1}).astype(int)\n",
    "print(mes.groupby('label').count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборка, очевидно, не сбалансирована - в ~6.5 раз меньше спама чем полезных сообщений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом проделаем векторизацию, иначе классификатор не сможет обработать то, что мы ему дадим, ведь у нас половина таблицы - это текст. Затем обучим классифиикатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.87      1.00      0.93      1205\n",
      "          1       0.00      0.00      0.00       188\n",
      "\n",
      "avg / total       0.75      0.87      0.80      1393\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\python\\anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mes_train, mes_test, label_train, label_test = train_test_split(mes['message'], mes['label'])\n",
    "#векторизация\n",
    "bow = CountVectorizer()\n",
    "bow.fit_transform(mes_train)\n",
    "train_bowed_mes = bow.transform(mes_train)\n",
    "test_bowed_mes = bow.transform(mes_test)\n",
    "#обучение DC\n",
    "clf = DummyClassifier(strategy='most_frequent', random_state=0)\n",
    "clf = clf.fit(train_bowed_mes, label_train)\n",
    "print(classification_report(label_test, clf.predict(test_bowed_mes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы видим, что DC не справляется с тем, чтобы определить спам. Поэтому работать мы с ним не будем"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация со знаками препинания. CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4835\n",
      "          1       0.97      0.98      0.98       737\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#векторизация\n",
    "bow_token_CV = CountVectorizer()\n",
    "bow_token_CV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_token_CV.transform(mes['message'])\n",
    "#наивный Байес\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "#выдача результатов\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация со знаками препинания. TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99      4958\n",
      "          1       0.82      1.00      0.90       614\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_token_TIV = TfidfVectorizer()\n",
    "bow_token_TIV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_token_TIV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация без знаков препинания. CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4829\n",
      "          1       0.97      0.97      0.97       743\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_token_P_CV = CountVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize)\n",
    "#Пытался сунуть другой токенайзер, но он отказался работать((((\n",
    "bow_token_P_CV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_token_P_CV.transform(mes['message'])\n",
    "naive_model1 = MultinomialNB()\n",
    "naive_model1.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model1.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация без знаков препинания. TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99      4961\n",
      "          1       0.82      1.00      0.90       611\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_token_P_TIV = TfidfVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize)\n",
    "bow_token_P_TIV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_token_P_TIV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизация со знаками препинания показала себя чуть лучше. Буквально на капельку. Можно сказать, что разницы нет вообще.\n",
    "Но в обоихъ случаях TfidfVectorizer() явно хуже, чем CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стемминг и лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг. CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4829\n",
      "          1       0.97      0.97      0.97       743\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_stem_CV = CountVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize)\n",
    "bow_stem_CV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_stem_CV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг. TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.963208815585 0.00590979736916\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99      4958\n",
      "          1       0.82      1.00      0.90       614\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_stem_TIV = TfidfVectorizer()\n",
    "bow_stem_TIV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_stem_TIV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(cv_results.mean(), cv_results.std())\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация. CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4829\n",
      "          1       0.97      0.97      0.97       743\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_lem_CV = CountVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize)\n",
    "bow_lem_CV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_lem_CV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лемматизация. TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99      4961\n",
      "          1       0.82      1.00      0.90       611\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_lem_TIV = TfidfVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize)\n",
    "bow_lem_TIV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_lem_TIV.transform(mes['message'])\n",
    "naive_model6 = MultinomialNB()\n",
    "naive_model6.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model6.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Между лемматизацией и стеммингом никакой разницы. При этом CountVectorizer() и TfidfVectorizer() сохраняют свои различия. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00      4820\n",
      "          1       0.98      0.98      0.98       752\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_SW_CV = CountVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize, stop_words='english')\n",
    "bow_SW_CV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_SW_CV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.97      0.99      4961\n",
      "          1       0.82      1.00      0.90       611\n",
      "\n",
      "avg / total       0.98      0.98      0.98      5572\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bow_SW_TIV = TfidfVectorizer(tokenizer=RegexpTokenizer(r'\\w+').tokenize)\n",
    "bow_SW_TIV.fit_transform(mes['message'])\n",
    "bowed_messages = bow_SW_TIV.transform(mes['message'])\n",
    "naive_model = MultinomialNB()\n",
    "naive_model.fit(bowed_messages, mes['label'])\n",
    "cv_results = cross_val_score(naive_model, bowed_messages, mes['label'], cv=10, scoring='accuracy')\n",
    "print(classification_report(naive_model.predict(bowed_messages), mes['label']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Впрочем, ничего нового."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Я искренне надеялся сделать побольше и сам, но увы. Я хотя бы попытался и разобрался в том, что изучил."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
